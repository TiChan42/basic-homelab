#!/bin/bash
# ============================================
# Home Assistant Backup Script
# Uses WebSocket API via ha_supervisor_api.py
# for reliable FULL backup creation & download.
# Numbered rotation (0 = newest) with validation.
#
# Creates FULL backups (not partial) to ensure
# all addon data (Z2M device database, Matter,
# etc.), HA core config, folders, and the
# recorder database are captured completely.
# ============================================
set -euo pipefail

REFRESH_TOKEN_FILE="{{ ha_refresh_token_file }}"
BACKUP_DIR_ROOT="{{ nas_backup_mount }}/homeassistant"
KEEP_COUNT={{ ha_backup_keep_count }}
MIN_BACKUP_SIZE={{ ha_backup_min_size }}
API_HELPER="{{ ha_supervisor_api_path | default('/home/homelab/basic-homelab/ansible-playbooks/services/102-homeassistant/files/ha_supervisor_api.py') }}"
LOG_TAG="ha-backup"

# Verify prerequisites
if [ ! -f "$REFRESH_TOKEN_FILE" ]; then
  logger -t "$LOG_TAG" "ERROR: Refresh token file not found at $REFRESH_TOKEN_FILE"
  exit 1
fi
REFRESH_TOKEN=$(cat "$REFRESH_TOKEN_FILE")

if [ ! -f "$API_HELPER" ]; then
  logger -t "$LOG_TAG" "ERROR: API helper not found at $API_HELPER"
  exit 1
fi

# Check NFS mount is available
if ! mountpoint -q "{{ nas_mount }}"; then
  logger -t "$LOG_TAG" "ERROR: NAS mount not available at {{ nas_mount }}"
  exit 1
fi

# List of HA instances (IP:NAME)
HA_INSTANCES=(
{% for item in groups['homeassistant'] %}
  "{{ hostvars[item]['ansible_host'] }}:{{ item }}"
{% endfor %}
)

# -----------------------------------------------
# Helper: obtain a fresh access token via refresh
# -----------------------------------------------
get_access_token() {
  local HA_URL="$1"
  local TOKEN_RESPONSE
  TOKEN_RESPONSE=$(curl -s -X POST "$HA_URL/auth/token" \
    -d "grant_type=refresh_token" \
    -d "client_id=$HA_URL/" \
    -d "refresh_token=$REFRESH_TOKEN")

  echo "$TOKEN_RESPONSE" | jq -r '.access_token // empty'
}

for instance in "${HA_INSTANCES[@]}"; do
  IP="${instance%%:*}"
  NAME="${instance##*:}"
  HA_URL="http://$IP:{{ ha_port | default('8123') }}"
  INSTANCE_TAG="ha-backup-$NAME"
  BACKUP_DIR="$BACKUP_DIR_ROOT/$NAME"
  BACKUP_PREFIX="$BACKUP_DIR/ha_backup"

  mkdir -p "$BACKUP_DIR"
  logger -t "$INSTANCE_TAG" "Starting backup for $NAME ($IP)..."

  # Obtain fresh access token
  HA_TOKEN=$(get_access_token "$HA_URL")
  if [ -z "$HA_TOKEN" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Failed to obtain access token. Skipping."
    continue
  fi

  # Check if HA is reachable
  HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
    -H "Authorization: Bearer $HA_TOKEN" \
    "$HA_URL/api/" || echo "000")

  if [ "$HTTP_CODE" != "200" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: HA API not reachable (HTTP $HTTP_CODE). Skipping."
    continue
  fi

  # Trigger FULL backup via Supervisor REST API (POST /backups/new/full).
  # The WebSocket 'backup/generate' command creates partial backups even
  # without include_* params. The Supervisor REST endpoint creates a true
  # full backup containing: HA core config, ALL addons with their data,
  # ALL folders (ssl, share, media, addons/local), and the recorder database.
  # This ensures everything needed for a complete restore is captured,
  # including Z2M's device database, coordinator backup, and Matter data.
  BACKUP_RESPONSE=$(python3 "$API_HELPER" "$HA_URL" "$HA_TOKEN" POST /backups/new/full \
    '{"name":"automated_nas_backup"}' 2>&1) || true

  # The Supervisor REST endpoint returns both slug and job_id.
  # The call blocks until the backup is created, so the slug is
  # immediately available – no need to poll for completion.
  BACKUP_SLUG=$(echo "$BACKUP_RESPONSE" | jq -r '.slug // empty' 2>/dev/null)
  BACKUP_JOB_ID=$(echo "$BACKUP_RESPONSE" | jq -r '.job_id // empty' 2>/dev/null)

  if [ -z "$BACKUP_SLUG" ] && [ -z "$BACKUP_JOB_ID" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Failed to trigger backup. Response: $BACKUP_RESPONSE"
    continue
  fi

  # If we got the slug directly (synchronous), use it.
  # Otherwise fall back to polling for completion.
  BACKUP_ID="$BACKUP_SLUG"

  if [ -z "$BACKUP_ID" ]; then
    logger -t "$INSTANCE_TAG" "Backup triggered (job: $BACKUP_JOB_ID). Waiting for completion..."

    # Wait for backup completion (poll backup/info every 10s, max 10 min)
    for i in $(seq 1 60); do
      sleep 10
      BACKUP_INFO=$(python3 "$API_HELPER" "$HA_URL" "$HA_TOKEN" ws backup/info 2>/dev/null) || continue
      BACKUP_STATE=$(echo "$BACKUP_INFO" | jq -r '.state // empty' 2>/dev/null)

      if [ "$BACKUP_STATE" = "idle" ]; then
        # Backup complete – find the newest backup
        BACKUP_ID=$(echo "$BACKUP_INFO" | jq -r '.backups | sort_by(.date) | reverse | .[0].backup_id // empty' 2>/dev/null)
        break
      fi
    done
  else
    logger -t "$INSTANCE_TAG" "Backup created (slug: $BACKUP_ID)."
  fi

  if [ -z "$BACKUP_ID" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Backup did not complete within timeout."
    continue
  fi

  logger -t "$INSTANCE_TAG" "Backup complete (ID: $BACKUP_ID). Downloading..."

  # Download backup via Supervisor API (REST works for download with auth token)
  TMP_FILE="$BACKUP_DIR/.ha_backup_downloading.tar"

  DOWNLOAD_HTTP=$(curl -s -o "$TMP_FILE" -w "%{http_code}" \
    -H "Authorization: Bearer $HA_TOKEN" \
    "$HA_URL/api/hassio/backups/$BACKUP_ID/download")

  # If REST proxy blocks the download, try direct Supervisor download
  if [ "$DOWNLOAD_HTTP" != "200" ]; then
    logger -t "$INSTANCE_TAG" "REST download failed (HTTP $DOWNLOAD_HTTP). Trying Supervisor API..."
    DOWNLOAD_RESP=$(python3 "$API_HELPER" "$HA_URL" "$HA_TOKEN" GET "/backups/$BACKUP_ID/download" 2>&1) || true
    # The supervisor/api websocket can't stream binary; use slug-based download
    # Try the hassio ingress download endpoint
    DOWNLOAD_HTTP=$(curl -s -o "$TMP_FILE" -w "%{http_code}" \
      -H "Authorization: Bearer $HA_TOKEN" \
      -H "X-Hassio-Key: $HA_TOKEN" \
      "$HA_URL/api/hassio/backups/$BACKUP_ID/download")
  fi

  # Validate: HTTP status
  if [ "$DOWNLOAD_HTTP" != "200" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Download failed (HTTP $DOWNLOAD_HTTP). Keeping previous backups."
    rm -f "$TMP_FILE"
    continue
  fi

  # Validate: file exists and has content
  if [ ! -s "$TMP_FILE" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Downloaded file is empty. Keeping previous backups."
    rm -f "$TMP_FILE"
    continue
  fi

  # Validate: minimum size
  FILE_SIZE=$(stat -c%s "$TMP_FILE" 2>/dev/null || stat -f%z "$TMP_FILE" 2>/dev/null || echo "0")
  if [ "$FILE_SIZE" -lt "$MIN_BACKUP_SIZE" ]; then
    logger -t "$INSTANCE_TAG" "ERROR: Backup too small (${FILE_SIZE} bytes, min ${MIN_BACKUP_SIZE}). Keeping previous backups."
    rm -f "$TMP_FILE"
    continue
  fi

  # Validate: valid tar archive
  if ! tar -tf "$TMP_FILE" > /dev/null 2>&1; then
    logger -t "$INSTANCE_TAG" "ERROR: Downloaded file is not a valid tar archive. Keeping previous backups."
    rm -f "$TMP_FILE"
    continue
  fi

  # All checks passed – overwrite protection
  if [ -f "${BACKUP_PREFIX}.0.tar" ]; then
    EXISTING_SIZE=$(stat -c%s "${BACKUP_PREFIX}.0.tar" 2>/dev/null || echo "0")
    if [ "$EXISTING_SIZE" -gt "$MIN_BACKUP_SIZE" ] && [ "$FILE_SIZE" -lt $(( EXISTING_SIZE / 2 )) ]; then
      logger -t "$INSTANCE_TAG" "WARNING: New backup (${FILE_SIZE} bytes) is less than half the existing (${EXISTING_SIZE} bytes). Skipping to prevent data loss."
      rm -f "$TMP_FILE"
      continue
    fi
  fi

  # Rotate numbered backups
  rm -f "${BACKUP_PREFIX}.$((KEEP_COUNT - 1)).tar"

  for (( i = KEEP_COUNT - 2; i >= 0; i-- )); do
    if [ -f "${BACKUP_PREFIX}.${i}.tar" ]; then
      mv -f "${BACKUP_PREFIX}.${i}.tar" "${BACKUP_PREFIX}.$((i + 1)).tar"
    fi
  done

  mv -f "$TMP_FILE" "${BACKUP_PREFIX}.0.tar"

  # Cleanup old backups inside HA (keep KEEP_COUNT)
  ALL_BACKUP_IDS=$(python3 "$API_HELPER" "$HA_URL" "$HA_TOKEN" ws backup/info 2>/dev/null | \
    jq -r "[.backups | sort_by(.date) | reverse | .[$KEEP_COUNT:][] | .backup_id] | .[]" 2>/dev/null) || true

  for OLD_ID in $ALL_BACKUP_IDS; do
    python3 "$API_HELPER" "$HA_URL" "$HA_TOKEN" ws backup/delete "{\"backup_id\":\"$OLD_ID\"}" 2>/dev/null || true
  done

  # Metadata
  echo "backup_date=$(date -Iseconds)" > "$BACKUP_DIR/backup_meta.txt"
  echo "backup_id=${BACKUP_ID}" >> "$BACKUP_DIR/backup_meta.txt"
  echo "hostname=${NAME}" >> "$BACKUP_DIR/backup_meta.txt"
  echo "file_size=${FILE_SIZE}" >> "$BACKUP_DIR/backup_meta.txt"
  echo "keep_count=${KEEP_COUNT}" >> "$BACKUP_DIR/backup_meta.txt"

  logger -t "$INSTANCE_TAG" "Backup completed (${FILE_SIZE} bytes) -> ${BACKUP_PREFIX}.0.tar"
done
